# mock data

## 目录

- [1. 用户输入 tell me about recent ai agent technology breakthrough ](#1-用户输入-tell-me-about-recent-ai-agent-technology-breakthrough-)
- [2. 模型输出（clarification qs）](#2-模型输出clarification-qs)
- [3. 用户输入更多细节](#3-用户输入更多细节)
- [4. 生成脑图](#4-生成脑图)

# 1. 用户输入 tell me about recent ai agent technology breakthrough&#x20;

# 2. 模型输出（clarification qs）

```text 
To better address your question about recent AI agent technology breakthroughs, I need to clarify a few key aspects:

1. Domain Focus: Are you interested in a specific application area (e.g., robotics, healthcare, customer service) or general advancements in AI agents?
2. Timeframe: How recent? The last 6 months, 1 year, or a broader period?
3. Technical Depth: Do you want high-level summaries (e.g., "AI agents can now do X") or detailed technical insights (e.g., architectures like Mixture of Experts)?
4. Type of Breakthrough: Are you looking for improvements in capabilities (e.g., planning, tool use), scalability, or real-world deployments?

If you’d prefer not to specify, I’ll assume a general scenario—covering notable advances in the past year across capabilities like multi-agent collaboration, memory/context handling, and real-world integration.
```


# 3. 用户输入更多细节

```markdown 
1. I want to focus on AI agent applications
2. Better the last 2 weeks or even 1 weeks
3. I want detailed technical insights
4. Both type is ok, but I need really important breakthrough
```


# 4. 生成脑图

- 生成的维度
  ```json 
  {
    "Technical Dimension": "A key technical development involves the integration of reinforcement learning with advanced neural architectures, such as transformers, allowing agents to better process and utilize temporal and spatial information. This was achieved by introducing feedback loops that enable the agent to recalibrate its decision-making model in real-time, addressing the challenge of state space complexity in dynamic environments.",
    "Application Dimension": "In practical terms, this breakthrough has elevated the effectiveness of AI in autonomous driving systems, where the agent can now handle unusual traffic scenarios without predetermined programming. Moreover, similar frameworks are being tested in robotic process automation (RPA), improving the accuracy and efficiency of tasks previously constrained by static rule-based systems.",
    "Scalability and Efficiency": "The integration of parallel processing techniques and optimization of algorithms through distributed computing architecture has made these systems scalable. These improvements decrease computational load and increase the efficiency of AI agents, making large-scale deployment feasible while reducing energy consumption.",
    "Impact Dimension": "This breakthrough is expected to significantly alter industries reliant on autonomous technologies, such as transportation, logistics, and manufacturing. Enhanced AI agents will contribute to higher safety standards, reduced operational costs, and improved adaptability to unforeseen challenges, providing a compelling case for further investment in AI infrastructure."
  }

  ```

- 对每个维度具体搜索得到的结果
  - 每个子元素有如下字段
  ```python 
  result_item = {
                      "url": item.url,
                      "id": item.id,
                      "title": item.title,
                      "score": item.score,
                      "publishedDate": item.published_date,
                      "author": item.author,
                      "text": item.text,
                      "summary": _get_summary(item.text[:16384])
                  }
  ```

  - Technical Dimension
  ```json 
  [
    {
      "url": "https://export.arxiv.org/pdf/2301.07608v1.pdf",
      "id": "https://export.arxiv.org/pdf/2301.07608v1.pdf",
      "title": "Human-Timescale Adaptation in an Open-Ended Task Space",
      "score": 0.38172483444213867,
      "publishedDate": "2023-01-18T15:39:21.000Z",
      "author": "Adaptive Agent Team,Jakob Bauer,Kate Baumli,Satinder Singh Baveja,Feryal Behbahani,Avishkar Bhoopchand,Nathalie Bradley-Schmieg,Michael Chang,Natalie A. Clay,Adrian Collister,Vibhavari Dasagi,Lucy González,Karol Gregor,Edward Hughes,Sheleem Kashem,Maria Loks-Thompson,Hannah Openshaw,Jack Parker-Holder,Shreya Pathak,Nicolas Perez-Nieves,Nemanja Rakicevic,Tim Rocktäschel,Yannick Schroecker,Jakub Sygnowski,Karl Tuyls,Sarah York,Alexander Zacherl,Lei Zhang",
      "text": "Human-Timescale Adaptation in an Open-Ended Task Space\nAdaptive Agents Team 1 1 DeepMindFoundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesisdriven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.\nFigure 1\n| Human timescale adaptation. Example trajectories of our agent (AdA) solving a held-out task in a complex 3D environment within minutes of test-time experience without any further agent training. Initial trials (Exploration) show a policy that uncovers hidden environment dynamics. After just seconds of test-time experience (Success), AdA finds a valid solution to the task. Later (Refinement), it improves this solution, gradually finding a more rewarding behaviour. The solid white lines show agent movement. The dashed coloured lines show the agent carrying an object of the corresponding colour. For a full description of the task, see Figure B.1. Videos of AdA's behaviour are available on our microsite and accompanying results reel.\nIntroduction\nThe ability to adapt in minutes is a defining characteristic of human intelligence and an important milestone on the path towards general intelligence. Given any level of bounded rationality, there will be a space of tasks in which it is impossible for agents to succeed by just generalising their policy zero-shot, but where progress is possible if the agent is capable of very fast in-context learning from feedback. To be useful in the real world, and in interaction with humans, our artificial agents should be capable of fast and flexible adaptation given only a few interactions, and should continue to adapt as more data becomes available. Operationalising this notion of adaptation, we seek to train an agent that, given few episodes in an unseen environment at test time, can accomplish a task that requires trial-and-error exploration and can subsequently refine its solution towards optimal behaviour. Meta-RL has been shown to be effective for fast in-context adaptation (e.g. Yu et al. (2020); Zintgraf (2022)). However, meta-RL has had limited success in settings where the reward is sparse and the task space is vast and diverse . Outside RL, foundation models in semisupervised learning have generated significant interest (Bommasani et al., 2021) due to their ability to adapt in few shots from demonstrations across a broad range of tasks. These models are designed to provide a strong foundation of general knowledge and skills that can be built upon and adapted to new situations via fine-tuning or prompting with demonstrations (Brown et al., 2020). Crucial to this success has been attention-based memory architectures like Transformers (Vaswani et al., 2017), which show power-law scaling in performance with the number of parameters (Tay et al., 2022). Figure 2 | Training our Adaptive Agent (AdA). We train a large Transformer model with meta-RL in XLand. During training, tasks are uniformly sampled, and subsequently filtered to produce an ever-changing training pool of tasks at the frontier of the agent's capabilities. After training on these tasks, the agent is capable of adapting to unseen hand-authored tasks as effectively and efficiently as humans.\nIn this work, we pave the way for training an RL foundation model; that is, an agent that has been pre-trained on a vast task distribution and that, at test time, can adapt few-shot to a broad range of downstream tasks. We introduce Adaptive Agent (AdA), an agent capable of human-timescale adaptation in a vast open-ended task space with sparse rewards. AdA does not require any prompts (Reed et al., 2022), fine-tuning (Lee et al., 2022) or access to offline datasets (Laskin et al., 2022;Reed et al., 2022). Instead, AdA exhibits hypothesis-driven exploratory behaviour, using information gained on-the-fly to refine its policy and to achieve close to optimal performance. AdA acquires knowledge efficiently, adapting in minutes on challenging held-out sparse-reward tasks in a partially-observable 3D environment with a first-person pixel observation. A human study confirms that the timescale of AdA's adaptation is comparable to that of trained human players. AdA's adaptation behaviour in a representative held-out task can be seen in Figure 1. AdA can also achieve improved performance through zero-shot prompting with first-person demonstrations, analogously to foundation models in the language domain.\nWe use Transformers as an architectural choice to scale in-context fast adaptation via model-based RL 2 (Duan et al., 2017;Melo, 2022;Wang et al., 2016). Foundation models typically require large, diverse datasets to achieve their generality (Brown et al., 2020;Mahajan et al., 2018;Schuhmann et al., 2022;Sun et al., 2017;Zhai et al., 2022). To make this possible in an RL setting, where agents collect their own data, we extend the recent XLand environment (OEL Team et al., 2021), producing a vast open-ended world with over 10 40 possible tasks. These tasks require a range of different online adaptation capabilities, including experimentation, navigation, coordination, division of labour and coping with irreversibility. Given the wide range of possible tasks, we make use of adaptive auto-curricula, which prioritise tasks at the frontier of an agent's capabilities (Jiang et al., 2021a;OEL Team et al., 2021). Finally, we make use of distillation (Schmitt et al., 2018), which enables scaling to models with over 500M parameters, to the best of our knowledge the largest model trained from scratch with RL at the time of publication (Ota et al., 2021). A high level overview of our method is shown in Figure 2.\nOur main contributions are as follows:\n• We introduce AdA, an agent capable of human-timescale adaptation in a wide range of challenging tasks. • We train AdA using meta-RL at scale in an open-ended task space with an automated curriculum. • We show that adaptation is influenced by memory architecture, curriculum, and the size and complexity of the training task distribution. • We produce scaling laws in both model size and memory, and demonstrate that AdA improves its performance with zero-shot first-person prompting.\nAdaptive Agent (AdA)\nTo achieve human timescale adaptation across a vast and diverse task space, we propose a general and scalable approach for memory-based meta-RL, producing an Adaptive Agent (AdA). We train and test AdA in XLand 2.0, an environment supporting procedural generation of diverse 3D worlds and multi-player games, with rich dynamics that necessitate adaptation. Our training method combines three key components: a curriculum to guide the agent's learning, a model-based RL algorithm to train agents with large-scale attention-based memory, and distillation to enable scaling. An overview of our approach is shown in Figure 2. In the following sections, we describe each component and how it contributes to efficient few-shot adaptation.\nOpen-ended task space: XLand 2.0\nIn order to demonstrate fast adaptation across an open-ended task space, we extend the procedurallygenerated 3D environment XLand (OEL Team et al., 2021), which we refer to here as XLand 1.0. In XLand, a task consists of a game, a world, and a list of co-player policies (if any). The game consists of a goal per player, defined as a boolean function (predicate) on the environment state. An agent receives reward if and only if the goal is satisfied. Goals are defined in a synthetic language, and the agent receives an encoding. The world specifies a static floor topology, objects the player can interact with, and spawn locations for players. The agent observes the world, and any co-players therein, via a first-person pixel observation. All fundamental details of the game, world and co-player system are For instance, in a task requiring experimentation, a player might be required to identify which objects can usefully combine, avoiding dead-ends, and then optimise the way in which they combine objects, like a toy version of experimental chemistry. Each task can be run for one or more trials, where the environment is reset between trials, but agent memory is not. Highlighted are two example tasks, Wrong Pair Disappears and Pass Over Wall Repeatedly, showing the goal, initial objects, production rules (\"rules\" in the figure) and how agents need to interact with them to solve the task. For full task descriptions see Appendix F.1.\ninherited from the original XLand; see OEL Team et al. (2021) for a full description and Appendix A.1 for details of the new features we added.\nXLand 2.0 extends XLand 1.0 with a system called production rules. Each production rule expresses an additional environment dynamic, leading to a much richer and more diverse array of different transition functions than in XLand 1.0. The production rules system can be thought of as a domainspecific language (DSL) to express this diverse array of dynamics. Each production",
      "summary": "The study introduces an Adaptive Agent (AdA) that demonstrates human-like timescale adaptation in a complex 3D task space, addressing limitations in reinforcement learning (RL) by employing a scalable attention-based memory architecture. AdA efficiently adapts to new tasks through hypothesis-driven exploration and exploits acquired knowledge, relying on meta-reinforcement learning, a large-scale memory architecture, and an automated curriculum that prioritizes frontier tasks. The training takes place in the XLand 2.0 environment, which is rich in diverse, procedurally generated tasks requiring skills like experimentation and coordination. This work achieves efficient few-shot adaptation and scaling, enabling AdA to perform well in open-ended domains, similar to foundation models in other AI fields."
    },
    {
      "url": "https://export.arxiv.org/pdf/2301.07608v1.pdf",
      "id": "https://export.arxiv.org/pdf/2301.07608v1.pdf",
      "title": "Human-Timescale Adaptation in an Open-Ended Task Space",
      "score": 0.38172483444213867,
      "publishedDate": "2023-01-18T15:39:21.000Z",
      "author": "Adaptive Agent Team,Jakob Bauer,Kate Baumli,Satinder Singh Baveja,Feryal Behbahani,Avishkar Bhoopchand,Nathalie Bradley-Schmieg,Michael Chang,Natalie A. Clay,Adrian Collister,Vibhavari Dasagi,Lucy González,Karol Gregor,Edward Hughes,Sheleem Kashem,Maria Loks-Thompson,Hannah Openshaw,Jack Parker-Holder,Shreya Pathak,Nicolas Perez-Nieves,Nemanja Rakicevic,Tim Rocktäschel,Yannick Schroecker,Jakub Sygnowski,Karl Tuyls,Sarah York,Alexander Zacherl,Lei Zhang",
      "text": "Human-Timescale Adaptation in an Open-Ended Task Space\nAdaptive Agents Team 1 1 DeepMindFoundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesisdriven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.\nFigure 1\n| Human timescale adaptation. Example trajectories of our agent (AdA) solving a held-out task in a complex 3D environment within minutes of test-time experience without any further agent training. Initial trials (Exploration) show a policy that uncovers hidden environment dynamics. After just seconds of test-time experience (Success), AdA finds a valid solution to the task. Later (Refinement), it improves this solution, gradually finding a more rewarding behaviour. The solid white lines show agent movement. The dashed coloured lines show the agent carrying an object of the corresponding colour. For a full description of the task, see Figure B.1. Videos of AdA's behaviour are available on our microsite and accompanying results reel.\nIntroduction\nThe ability to adapt in minutes is a defining characteristic of human intelligence and an important milestone on the path towards general intelligence. Given any level of bounded rationality, there will be a space of tasks in which it is impossible for agents to succeed by just generalising their policy zero-shot, but where progress is possible if the agent is capable of very fast in-context learning from feedback. To be useful in the real world, and in interaction with humans, our artificial agents should be capable of fast and flexible adaptation given only a few interactions, and should continue to adapt as more data becomes available. Operationalising this notion of adaptation, we seek to train an agent that, given few episodes in an unseen environment at test time, can accomplish a task that requires trial-and-error exploration and can subsequently refine its solution towards optimal behaviour. Meta-RL has been shown to be effective for fast in-context adaptation (e.g. Yu et al. (2020); Zintgraf (2022)). However, meta-RL has had limited success in settings where the reward is sparse and the task space is vast and diverse . Outside RL, foundation models in semisupervised learning have generated significant interest (Bommasani et al., 2021) due to their ability to adapt in few shots from demonstrations across a broad range of tasks. These models are designed to provide a strong foundation of general knowledge and skills that can be built upon and adapted to new situations via fine-tuning or prompting with demonstrations (Brown et al., 2020). Crucial to this success has been attention-based memory architectures like Transformers (Vaswani et al., 2017), which show power-law scaling in performance with the number of parameters (Tay et al., 2022). Figure 2 | Training our Adaptive Agent (AdA). We train a large Transformer model with meta-RL in XLand. During training, tasks are uniformly sampled, and subsequently filtered to produce an ever-changing training pool of tasks at the frontier of the agent's capabilities. After training on these tasks, the agent is capable of adapting to unseen hand-authored tasks as effectively and efficiently as humans.\nIn this work, we pave the way for training an RL foundation model; that is, an agent that has been pre-trained on a vast task distribution and that, at test time, can adapt few-shot to a broad range of downstream tasks. We introduce Adaptive Agent (AdA), an agent capable of human-timescale adaptation in a vast open-ended task space with sparse rewards. AdA does not require any prompts (Reed et al., 2022), fine-tuning (Lee et al., 2022) or access to offline datasets (Laskin et al., 2022;Reed et al., 2022). Instead, AdA exhibits hypothesis-driven exploratory behaviour, using information gained on-the-fly to refine its policy and to achieve close to optimal performance. AdA acquires knowledge efficiently, adapting in minutes on challenging held-out sparse-reward tasks in a partially-observable 3D environment with a first-person pixel observation. A human study confirms that the timescale of AdA's adaptation is comparable to that of trained human players. AdA's adaptation behaviour in a representative held-out task can be seen in Figure 1. AdA can also achieve improved performance through zero-shot prompting with first-person demonstrations, analogously to foundation models in the language domain.\nWe use Transformers as an architectural choice to scale in-context fast adaptation via model-based RL 2 (Duan et al., 2017;Melo, 2022;Wang et al., 2016). Foundation models typically require large, diverse datasets to achieve their generality (Brown et al., 2020;Mahajan et al., 2018;Schuhmann et al., 2022;Sun et al., 2017;Zhai et al., 2022). To make this possible in an RL setting, where agents collect their own data, we extend the recent XLand environment (OEL Team et al., 2021), producing a vast open-ended world with over 10 40 possible tasks. These tasks require a range of different online adaptation capabilities, including experimentation, navigation, coordination, division of labour and coping with irreversibility. Given the wide range of possible tasks, we make use of adaptive auto-curricula, which prioritise tasks at the frontier of an agent's capabilities (Jiang et al., 2021a;OEL Team et al., 2021). Finally, we make use of distillation (Schmitt et al., 2018), which enables scaling to models with over 500M parameters, to the best of our knowledge the largest model trained from scratch with RL at the time of publication (Ota et al., 2021). A high level overview of our method is shown in Figure 2.\nOur main contributions are as follows:\n• We introduce AdA, an agent capable of human-timescale adaptation in a wide range of challenging tasks. • We train AdA using meta-RL at scale in an open-ended task space with an automated curriculum. • We show that adaptation is influenced by memory architecture, curriculum, and the size and complexity of the training task distribution. • We produce scaling laws in both model size and memory, and demonstrate that AdA improves its performance with zero-shot first-person prompting.\nAdaptive Agent (AdA)\nTo achieve human timescale adaptation across a vast and diverse task space, we propose a general and scalable approach for memory-based meta-RL, producing an Adaptive Agent (AdA). We train and test AdA in XLand 2.0, an environment supporting procedural generation of diverse 3D worlds and multi-player games, with rich dynamics that necessitate adaptation. Our training method combines three key components: a curriculum to guide the agent's learning, a model-based RL algorithm to train agents with large-scale attention-based memory, and distillation to enable scaling. An overview of our approach is shown in Figure 2. In the following sections, we describe each component and how it contributes to efficient few-shot adaptation.\nOpen-ended task space: XLand 2.0\nIn order to demonstrate fast adaptation across an open-ended task space, we extend the procedurallygenerated 3D environment XLand (OEL Team et al., 2021), which we refer to here as XLand 1.0. In XLand, a task consists of a game, a world, and a list of co-player policies (if any). The game consists of a goal per player, defined as a boolean function (predicate) on the environment state. An agent receives reward if and only if the goal is satisfied. Goals are defined in a synthetic language, and the agent receives an encoding. The world specifies a static floor topology, objects the player can interact with, and spawn locations for players. The agent observes the world, and any co-players therein, via a first-person pixel observation. All fundamental details of the game, world and co-player system are For instance, in a task requiring experimentation, a player might be required to identify which objects can usefully combine, avoiding dead-ends, and then optimise the way in which they combine objects, like a toy version of experimental chemistry. Each task can be run for one or more trials, where the environment is reset between trials, but agent memory is not. Highlighted are two example tasks, Wrong Pair Disappears and Pass Over Wall Repeatedly, showing the goal, initial objects, production rules (\"rules\" in the figure) and how agents need to interact with them to solve the task. For full task descriptions see Appendix F.1.\ninherited from the original XLand; see OEL Team et al. (2021) for a full description and Appendix A.1 for details of the new features we added.\nXLand 2.0 extends XLand 1.0 with a system called production rules. Each production rule expresses an additional environment dynamic, leading to a much richer and more diverse array of different transition functions than in XLand 1.0. The production rules system can be thought of as a domainspecific language (DSL) to express this diverse array of dynamics. Each production",
      "summary": "DeepMind's research introduces the Adaptive Agent (AdA), a reinforcement learning (RL) model capable of adapting to novel 3D tasks in an open-ended environment as quickly as humans. AdA, developed using meta-RL, employs a large-scale attention-based memory architecture and an automated curriculum that prioritizes tasks at the edge of the agent's current capabilities, allowing it to learn efficiently and adapt on-the-fly without additional training. The agent can perform complex tasks in a partially observable environment and improve through hypothesis-driven exploration and first-person demonstrations. By extending the XLand environment to create a vast range of diverse tasks, AdA demonstrates significant progress in scalability and adaptability, leveraging key ingredients including memory architecture, curriculum design, and training task diversity. These advancements pave the way for more general and adaptable RL agents suitable for real-world applications."
    },
    {
      "url": "https://export.arxiv.org/pdf/2212.14538v2.pdf",
      "id": "https://export.arxiv.org/pdf/2212.14538v2.pdf",
      "title": "Transformer in Transformer as Backbone for Deep Reinforcement Learning",
      "score": 0.8005853295326233,
      "publishedDate": "2023-01-03T06:51:22.000Z",
      "author": "Hangyu Mao,Rui Zhao,Hao Chen,Jianye Hao,Yiqun Chen,Dong Liu,Junge Zhang,Xiao Zhang",
      "text": "Hangyu Mao\nRui Zhao\nHao Chen\nJianye Hao\nYiqun Chen\nDong Li\nJunge Zhang\nZhen Xiao\nTransformer in Transformer as Backbone for Deep Reinforcement Learning\nDesigning better deep networks and better reinforcement learning (RL) algorithms are both important for deep RL. This work focuses on the former. Previous methods build the network with several modules like CNN, LSTM and Attention. Recent methods combine the Transformer with these modules for better performance. However, it requires tedious optimization skills to train a network composed of mixed modules, making these methods inconvenient to use in practice. In this paper, we propose to design pure Transformer-based networks for deep RL, aiming at providing offthe-shelf backbones for both the online and offline settings. Specifically, the Transformer in Transformer (TIT) backbone is proposed, which cascades two Transformers in a very natural way: the inner one is used to process a single observation, while the outer one is responsible for processing the observation history. Combining both is expected to extract spatial-temporal representations for good decision-making. Experiments show that TIT can achieve satisfactory performance in different settings consistently.\nIntroduction\nDeep reinforcement learning (RL) has made great progress in various domains, such as mastering video games (Mnih et al., 2015), cooling data centers (Evans & Gao, 2016), and manipulating robotic arms (Kalashnikov et al., 2018). In general, a deep RL method mainly consists of two parts: the deep network that generates the action (and value), and the RL algorithm that trains the deep network.\nMost deep RL methods focus on the innovation of the RL algorithms. The representative methods include but are not performance varies greatly as shown by our experiments.\nBecause mastering the skills to train networks composed of hybrid modules is especially hard, we ask the following question in this paper: can we design deep networks for deep RL purely based on Transformers, so that we can use it as backbone off-the-shelf to achieve good performance, without being disturbed by the combination of other network modules and the tedious optimization skills?\nWe target at Transformers rather than other network modules due to the following reasons. Firstly, Transformer (Vaswani et al., 2017;Kenton & Toutanova, 2019;Brown et al., 2020) and Vision Transformer (Dosovitskiy et al., 2020;Liu et al., 2021;Han et al., 2021) are the most outstanding breakthroughs in natural language processing (NLP) and computer vision (CV), and they have demonstrated the practical effectiveness and scalability for processing sequential words and image patches (and more generally, sequential data). Secondly, the goal of deep RL is sequential decisionmaking based on sequential observations (and popularly, the observation is an image), which matches the ability of Transformer and Vision Transformer exactly. Lastly, reviewing previous deep RL methods like DQN, DRQN, DARQN and these discussed in Section 2, we find that (1) their deep networks are largely inspired by the breakthroughs in NLP and CV, and that (2) both NLP and CV breakthroughs will eventually result in better deep RL performance when properly applied, so do Transformers we believe.\nTo this end, we propose to explore two Transformer in Transformer (TIT) backbones. Specifically, both of the two TIT backbones are made up of an Inner Transformer and an Outer Transformer: the inner one is used to process a single observation at the observation patch level to learn a good observation representation that captures important spatial information in an observation, while the outer one is responsible for processing sequential observations (or tuples of return, observation, action in the learning paradigm of Decision Transformer (Chen et al., 2021b)) to capture important temporal information across multiple consecutive observations, and combining both is expected to extract spatial-temporal representations for better decision-making.\nThe two TIT backbones differ in the way how the two Transformers are cascaded. The Vanilla TIT shown in Figure 1 applies L Inner Transformer blocks before L Outer Transformer blocks. There is no information interaction between the two types of blocks. In contrast, the Enhanced TIT shown in Figure 2 builds its TIT block by the inner block and the outer block, and stacks L TIT blocks to form the TIT backbone. Therefore, it can fuse the spatial-temporal information in every TIT blocks, which may capture more suitable representations compared to Vanilla TIT.\nOur contributions are summarized as follows. (1) To our best knowledge, we are the first to show that pure Transformers can serve as the backbones for both the standard online and offline RL methods (e.",
      "summary": "This research proposes using pure Transformer-based networks as backbones for deep reinforcement learning (RL), aiming to simplify training by avoiding complex module combinations. The paper introduces the Transformer in Transformer (TIT) architecture, which utilizes an Inner Transformer for processing single observations and an Outer Transformer for handling observation sequences, enhancing spatial-temporal representation extraction for decision-making. Two TIT variants are explored: Vanilla TIT, which separates inner and outer processing, and Enhanced TIT, which integrates these processes more closely to better capture spatial-temporal information. Experiments demonstrate that the TIT approach achieves consistent and satisfactory performance across various RL settings, highlighting the efficacy of Transformers as standalone deep RL backbones."
    },
    {
      "url": "https://export.arxiv.org/pdf/2307.05979v1.pdf",
      "id": "https://export.arxiv.org/pdf/2307.05979v1.pdf",
      "title": "Transformers in Reinforcement Learning: A Survey",
      "score": 0.7847342491149902,
      "publishedDate": "2023-07-12T07:51:12.000Z",
      "author": "Pranav Agarwal,Aamer Abdul Rahman,Pierre-Luc St-Charles,Simon J. D. Prince,Samira Ebrahimi Kahou",
      "text": "Pranav Agarwal pranav.agarwal.1@ens.etsmtl.net\nÉcole De Technologie\nSupérieure / Mila\nCanada Aamer\nAbdul Rahman aamer.abdul-rahman.1@ens.etsmtl.net\nSimon J D Prince\nStcharles@mila\nSimon J D Prince\nÉcole de Technologie Supérieure/Mila\nCanada\nPIERRE-LUC ST-CHARLES\nMilaApplied\nML Research Team\nCanada\nUniversity of Bath\nUnited Kingdom\nÉcole de Technologie Supérieure/Mila/CIFAR\nPranav Agarwal, École de Technologie Supérieure/Mila\nAamer Abdul Rahman, École de Technologie Supérieure/Mila\nSAMIRA EBRAHIMI KAHOU\nPierre-Luc St-CharlesMontréal, Montréal, MilaQuébec, Québec, AppliedCanada, Canada, Canada\nML Research Team\nMontréalQuébec, Canada, pierreluc\nSamira Ebrahimi Kahou, École de Technologie Supérieure/Mila/CIFAR\nUniversity of Bath\nBath, MontréalQuébecUnited Kingdom, Canada\nTransformers in Reinforcement Learning: A Survey\n12 Jul 2023Authors' addresses: 2CCS Concepts: • Computing methodologies → Reinforcement learningNeural networks• General and reference → Surveys and overviews Additional Key Words and Phrases: reinforcement learning, transformers, representation learning, neural networks, literature survey\nTransformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers in RL, using visualization techniques and efficient training strategies. Often, the transformer architecture must be tailored to the specific needs of a given application. We present a broad overview of how transformers have been adapted for several applications, including robotics, medicine, language modeling, cloud computing, and combinatorial optimization. We conclude by discussing the limitations of using transformers in RL and assess their potential for catalyzing future breakthroughs in this field.\nIntroduction\nReinforcement learning (RL) is a learning paradigm that enables sequential decision-making by learning from feedback obtained through trial and error. It is usually formalized in terms of an Markov decision process (MDP), which provides a mathematical framework for modeling the interaction between an agent and its environment.\nMost RL algorithms optimize the agent's policy to select actions that maximize the expected cumulative reward. In deep RL, neural networks are used as function approximators for mapping the current state of the environment to the next action and for estimating future returns. This approach is beneficial when dealing with large or continuous state spaces that make tabular methods computationally expensive [169] and has been successful in challenging applications [6,90,133]. However, standard neural network architectures like convolutional neural networks (CNNs) and recurrent neural networks (RNNs) struggle with long-standing problems in RL. These problems include partial observability [37], inability to handle high-dimensional state and action spaces [10], and difficulty in handling long-term dependencies [22].\nPartial observability is a challenge in RL [104]; in the absence of complete information, the agent may be unable to make optimal decisions. A typical way to address this problem is to integrate the agent's input [160] over time using CNNs and RNNs. However, RNNs tend to forget information [142], while CNNs are limited in the number of past-time steps they can process [75]. Various strategies have been proposed to overcome this limitation, including gating mechanisms, gradient clipping, non-saturating activation functions, and manipulating gradient propagation paths [151]. Sometimes different data modalities, such as",
      "summary": "Transformers have significantly influenced fields such as natural language processing, computer vision, and robotics by enhancing performance compared to traditional neural networks. This survey examines the application of transformers in reinforcement learning (RL), where they help address challenges like unstable training, credit assignment, interpretability, and partial observability. The survey provides insights into RL, explores transformer properties and their suitability for RL challenges, and discusses their use in representation learning, transition, reward modeling, and policy optimization. It also highlights recent research efforts to improve transformer interpretability and efficiency and details transformer adaptations in applications like robotics, medicine, and cloud computing. The survey concludes by assessing the limitations and future potential of transformers in advancing RL."
    },
    {
      "url": "https://export.arxiv.org/pdf/2303.03982v2.pdf",
      "id": "https://export.arxiv.org/pdf/2303.03982v2.pdf",
      "title": "Structured State Space Models for In-Context Reinforcement Learning",
      "score": 0.3677259385585785,
      "publishedDate": "2023-11-23T16:02:22.000Z",
      "author": "Chris Xiaoxuan Lu,Yannick Schroecker,Albert Gu,Emilio Parisotto,Jakob Foerster,Satinder Singh,Feryal Behbahani",
      "text": "Chris Lu\nYannick Schroecker\nAlbert Gu\nEmilio Parisotto\nJakob Foerster\nSatinder Singh\nFeryal Behbahani\nStructured State Space Models for In-Context Reinforcement Learning\nStructured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers and performs better than LSTM models on a simple memory-based task. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper suggest that the S4 models are a strong contender for the default architecture used for in-context reinforcement learning.\nIntroduction\nStructured state space sequence (S4) models (Gu et al., 2021a) and their variants such as S5 (Smith et al., 2022) have recently achieved impressive results in long-range sequence modelling tasks, far outperforming other popular sequence models such as the Transformer (Vaswani et al., 2017) and LSTM (Hochreiter & Schmidhuber, 1997) on the Long-Range Arena benchmark (Tay et al., 2020). Notably, S4 was the first architecture to achieve a non-trivial result * Work done during an internship at DeepMind 1 Department of Engineering Sciences, University of Oxford, Oxford, United Kingdom 2 DeepMind. Correspondence to: Chris Lu. Preliminary work. on the difficult Path-X task, which requires the ability to handle extremely long-range dependencies of lengths 16k.\nFurthermore, S4 models display a number of desirable properties that are not directly tested by raw performance benchmarks. Unlike transformers, for which the per step runtime usually scales quadratically with the sequence length, S4 models have highly-scalable inference runtime performance, asymptotically using constant memory and time per step with respect to the sequence length. While LSTMs and other RNNs also have this property, S4 models are far more performant while also being parallelisable across the sequence dimension during training.\nWhile inference-time is normally not included when evaluating on sequence modelling benchmarks, it has a large impact on the scalability and wallclock-time for reinforcement learning (RL) because the agent uses inference to collect data from the environment. Thus, transformers usually have poor runtime performance in reinforcement learning (Parisotto & Salakhutdinov, 2021). Thus, while transformers have become the default architecture for many supervised sequence-modelling tasks (Vaswani et al., 2017), LSTMs are still widely-used for memory-based RL tasks (Ni et al., 2022).\nThe ability to efficiently model contexts that are orders of magnitude larger may enable new possibilities in RL. This is particularly applicable in meta-reinforcement learning (Meta-RL), in which the agent is trained to adapt across multiple environment episodes. One approach to Meta-RL, RL 2 (Duan et al., 2016;Wang et al., 2016), uses memory-based architectures, such as RNNs, to directly learn across these episodes, which can result in sequences that are thousands of steps long. Most instances of RL 2 approaches, however, are limited to narrow task distributions and short adaptation horizons because of their limited effective memory length and slow training speeds. S4 models would enable training across larger, more complex, task distributions by providing a more scalable and performant memory architecture, ultimately resulting in more general agents.\nUnfortunately, simply applying S4 models to reinforcement learning is challenging. This is because the most popular training paradigm in RL involves collecting fixed-length environment trajectories, which often cross episode bound-arXiv:2303.03982v2 [cs.\nLG] 9 Mar 2023 aries. RNNs handle episode boundaries by resetting the hidden state at those transitions when performing backpropagation through time. Unlike RNNs, S4 models cannot simply reset their hidden states within the sequence because they",
      "summary": "Structured state space sequence (S4) models have excelled in long-range sequence modeling tasks due to their efficient parallelisable training and fast inference speeds, outperforming models like Transformers and LSTMs. The study introduces a modified S4 variant that allows for parallel initialisation and resetting of hidden states, enabling its application in reinforcement learning (RL) tasks. This modified architecture outperforms LSTMs on simple memory tasks and proves effective in complex meta-learning tasks by handling long-range sequences and adapting to out-of-distribution tasks. Despite challenges in applying S4 models to RL due to episode boundary issues, their scalability and performance make them a compelling architecture for in-context RL, potentially surpassing traditional models in this field."
    },
    {
      "url": "https://export.arxiv.org/pdf/2301.03044v3.pdf",
      "id": "https://export.arxiv.org/pdf/2301.03044v3.pdf",
      "title": "A Survey on Transformers in Reinforcement Learning",
      "score": 0.3688111901283264,
      "publishedDate": "2023-09-20T21:12:31.000Z",
      "author": "Wenzhe Li,Hao Luo,Zichuan Lin,Chongjie Zhang,Zongqing Lu,Deheng Ye",
      "text": "Wenzhe Li\nTsinghua University\n2 Peking University 3 BAAI\nHao Luo\nZichuan Lin zichuanlin@tencent.com\nTencent Inc\nZongqing Lu zongqing.lu@pku.edu.cn\nWashington University in St.Louis\nA Survey on Transformers in Reinforcement Learning\n20 Sep 20238C94B6A7120491C3C8F0F113C6757A54arXiv:2301.03044v3[cs.LG]\nTransformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings.Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL.However, the evolution of Transformers in RL has not yet been well unraveled.In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.\nIntroduction\nReinforcement learning (RL) provides a mathematical formalism for sequential decision-making.By utilizing RL, we can acquire intelligent behaviors automatically.While RL has provided a general framework for learning-based control, deep neural networks, as a way of function approximation with high capacity, have been enabling significant progress along a wide range of domains (Silver et al., 2016;Vinyals et al., 2019;Ye et al., 2020a;b).\nWhile the generality of deep reinforcement learning (DRL) has achieved tremendous developments in recent years, the issue of sample efficiency prevents its widespread use in real-world applications.To address this issue, an effective mechanism is to introduce inductive biases into the DRL framework.One important inductive bias in DRL is the choice of function approximator architectures, such as the parameterization of neural networks for DRL agents.However, compared to efforts on architectural designs in supervised learning (SL), how to design architectures for DRL has remained less explored.Most existing works on architectures for RL are motivated by the success of the (semi-) supervised learning community.For instance, a common practice to deal with high-dimensional image-based input in DRL is to introduce convolutional neural networks (CNN) (LeCun et al., 1998;Mnih et al., 2015); another common practice to deal with partial observability is to introduce recurrent neural networks (RNN) (Hochreiter & Schmidhuber, 1997;Hausknecht & Stone, 2015).\nIn recent years, the Transformer architecture (Vaswani et al., 2017) has revolutionized the learning paradigm across a wide range of SL tasks (Devlin et al., 2018;Dosovitskiy et al., 2020;Dong et al., 2018) and demonstrated superior performance over CNN and RNN.Among its notable benefits, the Transformer architecture enables modeling long dependencies and has excellent scalability (Khan et al., 2022).Inspired by the success of SL, there has been a surge of interest in applying Transformers in RL, with the hope of carrying the benefits of Transformers to the RL field.\nThe use of Transformers in RL dates back to Zambaldi et al. (2018), where the self-attention mechanism is used for relational reasoning over structured state representations.Afterward, many researchers seek to apply self-attention for representation learning to extract relations between entities for better policy learning (Vinyals et al., 2019;Baker et al., 2019).Besides leveraging Transformers for state representation learning, prior works also use Transformers to capture multi-step temporal dependencies to deal with the issue of partial observability (Parisotto et al., 2020;Parisotto & Salakhutdinov, 2021).More recently, offline RL (Levine et al., 2020) has attracted attention due to its capability to leverage large-scale offline datasets.Motivated by offline RL, recent efforts have shown that the Transformer architecture can directly serve as a model for sequential decisions (Chen et al., 2021;Janner et al., 2021) and generalize to multiple tasks and domains (Lee et al., 2022;Carroll et al., 2022).\nThe purpose of this survey is to present the field of Transformers in Reinforcement Learning, denoted as \"Transformerbased RL\".Although Transformer has been considered one of the most popular models in SL research at present (Devlin et al., 2018;Dosovitskiy et al., 2020;Bommasani et al., 2021;Lu et al., 2021), it remains to be less explored in the RL community.In fact, compared with the SL domain, using Transformers in RL as function approximators faces unique challenges.First, the training data of RL is collected by an ever-changing policy during optimization, which induces non-stationarity for learning a Transformer.Second, existing RL algorithms are often highly sensitive to design choices in the training process, including network architectures and capacity (Henderson et al., 2018).Third, Transformer-based architectures often suffer from high computational and memory costs, making it expensive in both training and inference during the RL learning process.For example, in the case of AI for video game-playing, the training performance is closely related to the efficiency of sample generation, which is restricted by the computational cost of the RL policy network and value network (Ye et al., 2020a;Berner et al., 2019).Fourthly, compared to models that rely on strong inductive biases, Transformer models typically need a much larger amount of training data to achieve decent performance, which usually exacerbates the sample efficiency problem of RL.Despite all these challenges, Transformers are becoming essential tools in RL due to their high expressiveness and capability.However, they are utilized for various purposes stemming from orthogonal advances in RL, such as a) RL that requires strong representation or world model (e.g., RL with high-dimensional spaces and long horizon); b) RL as a sequence modeling problem; and c) Pre-training large-scale foundation models for RL.In this paper, we seek to provide a comprehensive overview of Transformer-based RL, including a taxonomy of current methods and the challenges.We also discuss future perspectives, as we believe the field of Transformer-based RL will play an important role in unleashing the potential impact of RL, and this survey could provide a starting point for those looking to leverage its potential.\nWe structure the paper as follows.Section 2 covers background on RL and Transformers, followed by a brief introduction on how these two are combined together.In Section 3, we describe the evolution of network architecture in RL and the challenges that prevent the Transformer architecture from being widely explored in RL for a long time.In Section 4, we provide a taxonomy of Transformers in RL and discuss representative existing methods.Finally, we summarize and point out potential future directions in Section 5.\nProblem Scope\nReinforcement Learning\nIn general, Reinforcement Learning (RL) considers learning in a Markov Decision Process (MDP) M = ⟨S, A, P, r, γ, ρ 0 ⟩, where S and A denote the state space and action space respectively, P (s ′ |s, a) is the transition dynamics, r(s, a) is the reward function, γ ∈ (0, 1) is the discount factor, and ρ 0 is the distribution of initial states.Typically, RL aims to learn a policy π(a|s) to maximize the expected discounted return\nJ(π) = E π,P,ρ0 [ t γ t r(s t , a t )].\nTo solve an RL problem, we need to tackle two different parts: learning to represent states and learning to act.The first part can benefit from inductive biases (e.g., CNN for image-based states, and RNN for non-Markovian tasks).The second part can be solved via behavior cloning (BC), model-free or model-based RL.In the following part, we introduce several specific RL problems related to advances in Transformers in RL.\nOffline RL.In offline RL (Levine et al., 2020), the agent cannot interact with the environment during training.Instead, it only has access to a static offline dataset D = {(s, a, s ′ , r)} collected by arbitrary policies.Without exploration, modern offline RL approaches (Fujimoto et al., 2019;Kumar et al., 2020;Yu et al., 2021b) constrain the learned policy close to the data distribution, to avoid out-of-distribution actions that may lead to overestimation.\nRecently, in parallel with typical value-based methods, one popular trend in offline RL is RL via Supervised Learning (RvS) (Emmons et al., 2021), which learns an outcome-conditioned policy to yield desired behavior via SL.\nGoal-conditioned RL.Goal-conditioned RL (GCRL) extends the standard RL problem to goal-augmented setting, where the agent aims to learn a goal-conditioned policy π(a|s, g) that can reach multiple goals.Prior works propose to use various techniques, such as hindsight relabeling (Andrychowicz et al., 2017), universal value function (Schaul et al., 2015), and self-imitation learning (Ghosh et al., 2019), to improve the generalization and sample efficiency of GCRL.GCRL is quite flexible as there are diverse choices of goals.We refer readers to (Liu et al., 2022) for a detailed discussion around this topic.\nModel-based RL.In contrast to model-free RL which directly learns the policy and value functions, model-based RL learns an auxiliary dynamic model of the environment.Such a model can be directly used for planning (Schrittwieser et al., 2020), or generating imaginary trajectories to enlarge the training data for any model-free algorithm (Hafner et al., 2019).Learning a model is non-trivial, especially in large or partially observed environments where we first need to construct the representation of the state.Some recent methods propose to use latent dynamics (Hafner et al., 2019) or value models (Schrittwieser et al., 2020) to address these challenges and improve the sample efficiency of RL.\nTransformers\nTransformer (Vaswani et al., 2017) is one of the most effective and scalable neural networks to model sequential data.\nThe key idea of Transformer is to incorporate self-attention mechanism, which could capture dependencies with",
      "summary": "The paper reviews the application of Transformer architectures within the field of reinforcement learning (RL), a domain traditionally dominated by convolutional and recurrent neural networks. Although Transformers have revolutionized natural language processing and computer vision, their integration into RL presents unique challenges due to RL's dynamic data collection and training sensitivities. The survey categorizes existing methods using Transformers in RL, examining their potential benefits and limitations, such as improving state representation and handling sequence modeling in various RL settings, including offline and goal-conditioned RL. The authors discuss issues like high computation costs and sample inefficiency, yet highlight Transformers' potential in advancing RL, particularly for tasks requiring strong representations or long-term planning. The paper concludes by outlining future directions for Transformer-based approaches in RL."
    },
    {
      "url": "https://export.arxiv.org/pdf/2304.08487v1.pdf",
      "id": "https://export.arxiv.org/pdf/2304.08487v1.pdf",
      "title": "Hyper-Decision Transformer for Efficient Online Policy Adaptation",
      "score": 0.3635174632072449,
      "publishedDate": "2023-04-17T17:59:32.000Z",
      "author": "Min Xu,Yuchen Lu,Yikang Shen,Shun Zhang,Dongya Zhao,Chuang Gan",
      "text": "Mengdi Xu\nCarnegie Mellon University\nYuchen Lu\nUniversity of Montreal\nMila\nYikang Shen\nMIT-IBM Watson AI Lab\nShun Zhang\nMIT-IBM Watson AI Lab\nDing Zhao\nCarnegie Mellon University\nChuang Gan\nMIT-IBM Watson AI Lab\nUMass Amherst\nHYPER-DECISION TRANSFORMER FOR EFFICIENT ON- LINE POLICY ADAPTATION\nPublished as a conference paper at ICLR 2023\nDecision Transformers (DT) have demonstrated strong performances in offline reinforcement learning settings, but quickly adapting to unseen novel tasks remains challenging. To address this challenge, we propose a new framework, called Hyper-Decision Transformer (HDT), that can generalize to novel tasks from a handful of demonstrations in a data-and parameter-efficient manner. To achieve such a goal, we propose to augment the base DT with an adaptation module, whose parameters are initialized by a hyper-network. When encountering unseen tasks, the hyper-network takes a handful of demonstrations as inputs and initializes the adaptation module accordingly. This initialization enables HDT to efficiently adapt to novel tasks by only fine-tuning the adaptation module. We validate HDT's generalization capability on object manipulation tasks. We find that with a single expert demonstration and fine-tuning only 0.5% of DT parameters, HDT adapts faster to unseen tasks than fine-tuning the whole DT model. Finally, we explore a more challenging setting where expert actions are not available, and we show that HDT outperforms state-of-the-art baselines in terms of task success rates by a large margin. Demos are available on our project page. 1\nINTRODUCTION\nBuilding an autonomous agent capable of generalizing to novel tasks has been a longstanding goal of artificial intelligence. Recently, large transformer models have shown strong generalization capability on language understanding when fine-tuned with limited data (Brown et al., 2020;Wei et al., 2021). Such success motivates researchers to apply transformer models to the regime of offline reinforcement learning (RL) (Chen et al., 2021;Janner et al., 2021). By scaling up the model size and leveraging large offline datasets from diverse training tasks, transformer models have shown to be generalist agents successfully solving multiple games with a single set of parameters (Reed et al., 2022;Lee et al., 2022). Despite the superior performance in the training set of tasks, directly deploying these pre-trained agents to novel unseen tasks would still lead to suboptimal behaviors.\nOne solution is to leverage the handful of expert demonstrations from the unseen tasks to help policy adaptation, and this has been studied in the context of meta imitation learning (meta-IL) (Duan et al., 2017;Reed et al., 2022;Lee et al., 2022). In order to deal with the discrepancies between the training and testing tasks, these works focus on fine-tuning the whole policy model with either expert demonstrations or online rollouts from the test environments. However, with the advent of large pre-trained transformers, it is computationally expensive to fine-tune the whole models, and it is unclear how to perform policy adaptation efficiently (Figure 1 (a)). We aim to fill this gap in this work by proposing a more parameter-efficient solution.\nMoreover, previous work falls short in a more challenging yet realistic setting where the target tasks only provide demonstrations without expert actions. This is similar to the state-only imitation learning or Learning-from-Observation (LfO) settings (Torabi et al., 2019;Radosavovic et al., 2021), where expert actions are unavailable, and therefore we term this setting as meta Learning-from-Observation (meta-LfO). As a result, we aim to develop a more general method that can address both meta-IL and meta-LfO settings. Figure 1: Efficient online policy adaptation of pre-trained transformer models with few-shot demonstrations. To facilitate data efficiency, we introduce a demonstration-conditioned adaptation module that helps leverage prior knowledge in the demonstration and guide exploration. When adapting to novel tasks, we only fine-tune the adaptation module to maintain parameter efficiency.\nThe closest work to ours is Prompt-DT (Xu et al., 2022), which proposes to condition the model behavior in new environments on a few demonstrations as prompts. While the method is originally evaluated for meta-IL, the flexibility of the prompt design also allows this method to be useful for meta-LfO. However, we find that Prompt-DT hardly generalizes to novel environments (as",
      "summary": "The paper introduces the Hyper-Decision Transformer (HDT), a novel framework designed to enhance the adaptability of Decision Transformers (DT) in reinforcement learning settings with novel, unseen tasks. HDT incorporates an adaptation module whose parameters are initialized by a hyper-network, optimizing performance by fine-tuning only a small portion of DT's parameters. This approach is validated in object manipulation tasks and demonstrates faster task adaptation with just one expert demonstration compared to whole DT model fine-tuning. In scenarios where expert actions are unavailable, HDT surpasses existing methods significantly in task success rates. The paper highlights HDT's efficiency in both meta imitation learning (meta-IL) and meta Learning-from-Observation (meta-LfO) settings, improving upon previous methods like Prompt-DT by being more generalizable to new environments."
    },
    {
      "url": "https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/",
      "id": "https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/",
      "title": "A generalist AI agent for 3D virtual environments",
      "score": 0.38061344623565674,
      "publishedDate": "2025-04-24T00:00:00.000Z",
      "author": "SIMA Team",
      "text": "Research\n\n# A generalist AI agent for 3D virtual environments\n\nPublished13 March 2024Authors\n\nSIMA Team\n\n![](https://lh3.googleusercontent.com/2GNumOaJCB48RQIFbwJmmZro-AFdBebufxvY_ZkSdUs9RQ-0nSTgBMXuhUdIE5zpPknqevL4ZyP44PLOpJlg0U0ArlOCcJHfoOagzSnZZoXLnq7hdQ=w1072-h603-n-nu)\n\nWe present new research on a Scalable Instructable Multiworld Agent (SIMA) that can follow natural-language instructions to carry out tasks in a variety of video game settings\n\nVideo games are a key proving ground for artificial intelligence (AI) systems. Like the real world, games are rich learning environments with responsive, real-time settings and ever-changing goals.\n\nFrom our [early work with Atari games](https://www.nature.com/articles/nature14236/), through to our [AlphaStar](https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/) system that plays StarCraft II at human-grandmaster level, Google DeepMind has a long history in AI and games.\n\nToday, we’re announcing a new milestone - shifting our focus from individual games towards a general, instructable game-playing AI agent.\n\nIn a new [technical report](https://arxiv.org/abs/2404.10179), we introduce SIMA, short for Scalable Instructable Multiworld Agent, a generalist AI agent for 3D virtual settings. We partnered with game developers to train SIMA on a variety of video games. This research marks the first time an agent has demonstrated it can understand a broad range of gaming worlds, and follow natural-language instructions to carry out tasks within them, as a human might.\n\nThis work isn't about achieving high game scores. Learning to play even one video game is a technical feat for an AI system, but learning to follow instructions in a variety of game settings could unlock more helpful AI agents for any environment. Our research shows how we can translate the capabilities of advanced AI models into useful, real-world actions through a language interface. We hope that SIMA and other agent research can use video games as sandboxes to better understand how AI systems may become more helpful.\n\n## Learning from video games\n\nPause videoPlay video\n\nWe collaborated with eight game studios to train and test SIMA on nine different video games.\n\nTo expose SIMA to many environments, we’ve built a number of partnerships with game developers for our research. We collaborated with eight game studios to train and test SIMA on nine different video games, such as _No Man’s Sky_ by Hello Games and _Teardown_ by Tuxedo Labs. Each game in SIMA’s portfolio opens up a new interactive world, including a range of skills to learn, from simple navigation and menu use, to mining resources, flying a spaceship, or crafting a helmet.\n\nWe also used four research environments - including a new environment we built with [Unity](https://deepmind.google/discover/blog/using-unity-to-help-solve-intelligence/) called the Construction Lab, where agents need to build sculptures from building blocks which test their object manipulation and intuitive understanding of the physical world.\n\nBy learning from different gaming worlds, SIMA captures how language ties in with game-play behavior. Our first approach was to record pairs of human players across the games in our portfolio, with one player watching and instructing the other. We also had players play freely, then rewatch what they did and record instructions that would have led to their game actions.\n\n![](https://lh3.googleusercontent.com/sZ-76rwruMwufnHckkI7saPxuFc9kjX48x951LogZ_ADVauhANXXYCBbhhqEiDi2OkILF5Tuc8cRDPsyYeg3leEMUM4_GVn3Jo8e4n4YypgUZ_iKjw=w1070)\n\nSIMA comprises pre-trained vision models, and a main model that includes a memory and outputs keyboard and mouse actions.\n\n## SIMA: a versatile AI agent\n\nSIMA is an AI agent that can perceive and understand a variety of environments, then take actions to achieve an instructed goal. It comprises a model designed for precise image-language mapping and a video model that predicts what will happen next on-screen. We finetuned these models on training data specific to the 3D settings in the SIMA portfolio.\n\nOur AI agent doesn’t need access to a game's source code, nor bespoke APIs. It requires just two inputs: the images on screen, and simple, natural-language instructions provided by the user. SIMA uses keyboard and mouse outputs to control the games’ central character to carry out these instructions. This simple interface is what humans use, meaning SIMA can potentially interact with any virtual environment.\n\nThe current version of SIMA is evaluated across 600 basic skills, spanning navigation (e.g. \"turn left\"), object interaction (\"climb the ladder\"), and menu use (\"open the map\"). We’ve trained SIMA to perform simple tasks that can be completed within about 10 seconds.\n\nPause videoPlay video\n\nSIMA was evaluated across 600 basic skills, spanning navigation, object interaction, and menu use.\n\nWe want our future agents to tackle tasks that require high-level strategic planning and multiple sub-tasks to complete, such as “Find resources and build a camp”. This is an important goal for AI in general, because while Large Language Models have given rise to powerful systems that can capture knowledge about the world and generate plans, they currently lack the ability to take actions on our behalf.\n\n## Generalizing across games and more\n\nWe show an agent trained on many games was better than an agent that learned how to play just one. In our evaluations, SIMA agents trained on a set of nine 3D games from our portfolio significantly outperformed all specialized agents trained solely on each individual one. What’s more, an agent trained in all but one game performed nearly as well on that unseen game as an agent trained specifically on it, on average. Importantly, this ability to function in brand new environments highlights SIMA’s ability to generalize beyond its training. This is a promising initial result, however more research is required for SIMA to perform at human levels in both seen and unseen games.\n\nOur results also show that SIMA’s performance relies on language. In a control test where the agent was not given any language training or instructions, it behaves in an appropriate but aimless manner. For example, an agent may gather resources, a frequent behavior, rather than walking where it was instructed to go.\n\n![](https://deepmind.google/api/blob/website/images/Sima_Fig4.original.svg)\n\nWe evaluated SIMA’s ability to follow instructions to complete nearly 1500 unique in-game tasks, in part using human judges. As our baseline comparison, we use the performance of environment-specialized SIMA agents (trained and evaluated to follow instructions within a single environment). We compare this performance with three types of generalist SIMA agents, each trained across multiple environments.\n\n## Advancing AI agent research\n\nSIMA’s results show the potential to develop a new wave of generalist, language-driven AI agents. This is early-stage research and we look forward to further building on SIMA across more training environments and incorporating more capable models.\n\nAs we expose SIMA to more training worlds, the more generalizable and versatile we expect it to become. And with more advanced models, we hope to improve SIMA’s understanding and ability to act on higher-level language instructions to achieve more complex goals.\n\nUltimately, our research is building towards more general AI systems and agents that can understand and safely carry out a wide range of tasks in a way that is helpful to people online and in the real world.\n\n## Learn more about SIMA\n\n- [Read our technical report](https://arxiv.org/abs/2404.10179)\n\nWe would like to thank all the paper authors: Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, Stephanie Chan, Jeff Clune, Adrian Collister, Vikki Copeman, Alex Cullum, Ishita Dasgupta, Julia Di Trapani, Yani Donchev, Martin Engelcke, Ryan Faulkner, Frankie Garcia, Charles Gbadamosi, Zhitao Gong, Lucy Gonzales, Karol Gregor, Kshitij Gupta, Arne Olav Hallingstad, Tim Harley, Sam Haves, Felix Hill, Ed Hirst, Drew Hudson, Jony Hudson, Steph Hughes-Fitt, Danilo J. Rezende, Mimi Jasarevic, Laura Kampis, Rosemary Ke, Thomas Keck, Junkyung Kim, Oscar Knagg, Kavya Kopparapu, Andrew Lampinen, Rory Lawton, Shane Legg, Alexander Lerchner, Marjorie Limont, Yulan Liu, Maria Loks-Thompson, Joseph Marino, Kathryn Martin Cussons, Loic Matthey, Siobhan Mcloughlin, Piermaria Mendolicchio, Hamza Merzic, Anna Mitenkova, Alexandre Moufarek, Valeria Oliveira, Yanko Oliveira, Hannah Openshaw, Renke Pan, Aneesh Pappu, Alex Platonov, Ollie Purkiss, David Reichert, John Reid, Pierre Harvey Richemond, Tyson Roberts, Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars, Daniel P. Sawyer, Tim Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison Tam, Tayfun Terzi, Davide Vercelli, Bojan Vujatovic, Marcus Wainwright, Jane X. Wang, Zhengdong Wang, Daan Wierstra, Duncan Williams, Nathaniel Wong, Sarah York and Nick Young.\n\nSpecial thanks to all of the game developers who partnered with us: Coffee Stain (Valheim, Satisfactory, Goat Simulator 3) _,_ Foulball Hangover (Hydroneer) _,_ Hello Games (No Man's Sky) _,_ Keen Software House (Space Engineers) _,_ RubberbandGames (Wobbly Life) _,_ Strange Loop Games (Eco) _and_ Tuxedo Labs & Saber Interactive (Teardown).\n\n## Related posts\n\n[View all posts](https://deepmind.google/discover/blog/)\n\n- [![](https://lh3.googleusercontent.com/6DSrkFaInWqKD1eN4IJJN31ZRa3LW447A1ZYoK19FDzJGSLD5dlVw1rJRf52O_dmQUDq11XqYsiqMR8uFDnWLWGkl8xFY5KXYxD7LvQNPvTEuR_h=w400-h225-n-nu)\\\n\\\nResearch\\\n\\\nBuilding interactive agents in video game worlds\\\n\\\nMost artificial intelligence (AI) researchers now believe that writing computer code which can capture the nuances of situated interactions is impossible. Alternatively, modern machine learning...\\\n\\\n23 November 2022](https://deepmind.google/discover/blog/building-interactive-agents-in-video-game-worlds/)\n- [![](https://lh3.googleusercontent.com/5naqX6a3RVUtTU5y093Y0-9HNnDAijtGJZ_kzC-tcTDzTsDfkHFin1g-JqTwAqtogPKrQ2JnpoFjZL-BjXytT6iiD84wPbjXrFMOG6JpIChImS0N=w400-h225-n-nu)\\\n\\\nResearch\\\n\\\nAlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning\\\n\\\nAlphaStar is the first AI to reach the top league of a widely popular esport without any game restrictions. This January, a preliminary version of AlphaStar challenged two of the world's top...\\\n\\\n30 October 2019](https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/)\n- [![](https://lh3.googleusercontent.com/e0YojKfzZav4xp83-C1MhDxl34cHlHL32JjYrq3IUxq6UgR6SBR8FyPRDyy4SHTq5Q2D2rBKUijyLLhzn-v7_sIM52HMboN2XVieNOrCpUIUs46DHA=w400-h225-n-nu)\\\n\\\nResearch\\\n\\\nGenerally capable agents emerge from open-ended play\\\n\\\nIn recent years, artificial intelligence agents have succeeded in a range of complex game environments. For instance, AlphaZero beat world-champion programs in chess, shogi, and Go after starting...\\\n\\\n27 July 2021](https://deepmind.google/discover/blog/generally-capable-agents-emerge-from-open-ended-play/)\n- [Research\\\n\\\nUsing Unity to Help Solve Intelligence\\\n\\\nWe present our use of Unity, a widely recognised and comprehensive game engine, to create more diverse, complex, virtual simulations. We describe the concepts and components developed to simplify...\\\n\\\n18 November 2020](https://deepmind.google/discover/blog/using-unity-to-help-solve-intelligence/)\n- [Research\\\n\\\nMaking Efficient Use of Demonstrations to Solve Hard Exploration Problems\\\n\\\nThis paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We...\\\n\\\n5 September 2019](https://deepmind.google/discover/blog/making-efficient-use-of-demonstrations-to-solve-hard-exploration-problems/)",
      "summary": "Google DeepMind has developed SIMA, a Scalable Instructable Multiworld Agent, capable of executing tasks in diverse 3D virtual environments based on natural-language instructions. Unlike traditional game-specific AI, SIMA is generalized across various video games, utilizing partnerships with eight game studios for training, making it versatile and adaptable. The agent uses image-based inputs and simple instructions to perform tasks, highlighting its potential to understand and execute real-world actions. Evaluations showed SIMA's capability to generalize across different games, performing effectively in unseen environments by following linguistic prompts. This research suggests a promising trajectory towards more sophisticated, generalist AI agents that could be beneficial in various applications."
    },
    {
      "url": "http://arxiv.org/abs/2312.14878",
      "id": "http://arxiv.org/abs/2312.14878",
      "title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
      "score": 0.36954665184020996,
      "publishedDate": "2023-12-22T17:57:57.000Z",
      "author": "Christianos; Filippos; Papoudakis; Georgios; Zimmer; Matthieu; Coste; Thomas; Wu; Zhihao; Chen; Jingxuan; Khandelwal; Khyati; Doran; James; Feng; Xidong; Liu; Jiacheng; Xiong; Zheng; Luo; Yicheng; Hao; Jianye; Shao; Kun; Bou-Ammar; Haitham; Wang; Jun",
      "text": "http://arxiv.org/abs/2312.14878\nPangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning\n2023-12-22T00:00:00Z\n[Submitted on 22 Dec 2023]\n \n Authors: Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, Zheng Xiong, Yicheng Luo, Jianye Hao, Kun Shao, Haitham Bou-Ammar, Jun Wang \n View PDF \n Abstract: A key method for creating Artificial Intelligence (AI) agents is Reinforcement Learning (RL). However, constructing a standalone RL policy that maps perception to action directly encounters severe problems, chief among them being its lack of generality across multiple tasks and the need for a large amount of training data. The leading cause is that it cannot effectively integrate prior information into the perception-action cycle when devising the policy. Large language models (LLMs) emerged as a fundamental way to incorporate cross-domain knowledge into AI agents but lack crucial learning and adaptation toward specific decision problems. This paper presents a general framework model for integrating and learning structured reasoning into AI agents&#39; policies. Our methodology is motivated by the modularity found in the human brain. The framework utilises the construction of intrinsic and extrinsic functions to add previous understandings of reasoning structures. It also provides the adaptive ability to learn models inside every module or function, consistent with the modular structure of cognitive processes. We describe the framework in-depth and compare it with other AI pipelines and existing frameworks. The paper explores practical applications, covering experiments that show the effectiveness of our method. Our results indicate that AI agents perform and adapt far better when organised reasoning and prior knowledge are embedded. This opens the door to more resilient and general AI agent systems.\n \n \n Submission history From: Filippos Christianos [ view email] [v1] \nFri, 22 Dec 2023 17:57:57 UTC (1,481 KB) \n",
      "summary": "The paper discusses Pangu-Agent, a framework designed to enhance AI agents' abilities by integrating structured reasoning into their policies. Traditional Reinforcement Learning (RL) struggles with generality across various tasks and requires extensive training data due to its direct perception-action mapping. The authors propose a modular approach inspired by the human brain, incorporating intrinsic and extrinsic functions to utilize prior reasoning structures, thereby improving adaptability. By embedding organised reasoning and prior knowledge, AI agents can perform and adapt more effectively. The paper details the framework, compares it to existing models, and presents experimental results highlighting its effectiveness, suggesting a pathway for creating more robust and versatile AI systems."
    },
    {
      "url": "https://arxiv.org/abs/2408.03615",
      "id": "https://arxiv.org/abs/2408.03615",
      "title": "Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks",
      "score": 0.374385803937912,
      "publishedDate": "2024-10-21T11:05:27.000Z",
      "author": "Li; Zaijing; Xie; Yuquan; Shao; Rui; Chen; Gongwei; Jiang; Dongmei; Nie",
      "text": "\n View PDF \n HTML (experimental) \nBuilding a general-purpose agent is a long-standing vision in the field of artificial intelligence. Existing agents have made remarkable progress in many domains, yet they still struggle to complete long-horizon tasks in an open world. We attribute this to the lack of necessary world knowledge and multimodal experience that can guide agents through a variety of long-horizon tasks. In this paper, we propose a Hybrid Multimodal Memory module to address the above challenges. It 1) transforms knowledge into Hierarchical Directed Knowledge Graph that allows agents to explicitly represent and learn world knowledge, and 2) summarises historical information into Abstracted Multimodal Experience Pool that provide agents with rich references for in-context learning. On top of the Hybrid Multimodal Memory module, a multimodal agent, Optimus-1, is constructed with dedicated Knowledge-guided Planner and Experience-Driven Reflector, contributing to a better planning and reflection in the face of long-horizon tasks in Minecraft. Extensive experimental results show that Optimus-1 significantly outperforms all existing agents on challenging long-horizon task benchmarks, and exhibits near human-level performance on many tasks. In addition, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 exhibits strong generalization with the help of the Hybrid Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.\n \n \n Submission history From: Zaijing Li [ view email] [v1] \nWed, 7 Aug 2024 08:16:32 UTC (48,016 KB) \n ||||I|||| Grab your spot!\nWant to see access to research regardless of disability? Sign up for the arXiv Accessibility Forum in September and Learn more.\nSign Up\nSkip to main content\nGrab your spot at the free arXiv Accessibility Forum\nForum Schedule\nWe gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\n> cs > arXiv:2408.03615\nHelp | Advanced Search\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\nGO\nquick links\n* Login\n* Help Pages\n* About\nComputer Science > Artificial Intelligence\narXiv:2408.03615 (cs)\n[Submitted on 7 Aug 2024]\nTitle: Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks\nAuthors: Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie\nView a PDF of the paper titled Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks, by Zaijing Li and 5 other authors\nView PDF HTML (experimental)\nAbstract: Building a general-purpose agent is a long-standing vision in the field of artificial intelligence. Existing agents have made remarkable progress in many domains, yet they still struggle to complete long-horizon tasks in an open world. We attribute this to the lack of necessary world knowledge and multimodal experience that can guide agents through a variety of long-horizon tasks. In this paper, we propose a Hybrid Multimodal Memory module to address the above challenges. It 1) transforms knowledge into Hierarchical Directed Knowledge Graph that allows agents to explicitly represent and learn world knowledge, and 2) summarises historical information into Abstracted Multimodal Experience Pool that provide agents with rich references for in-context learning. On top of the Hybrid Multimodal Memory module, a multimodal agent, Optimus-1, is constructed with dedicated Knowledge-guided Planner and Experience-Driven Reflector, contributing to a better planning and reflection in the face of long-horizon tasks in Minecraft. Extensive experimental results show that Optimus-1 significantly outperforms all existing agents on challenging long-horizon task benchmarks, and exhibits near human-level performance on many tasks. In addition, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 exhibits strong generalization with the help of the Hybrid Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.\nComments: 30 pages, 13 figures\nSubjects: Artificial Intelligence (cs.AI) ; Computation and Language (cs.CL)\nCite as: arXiv:2408.03615 [cs.AI]\n(or arXiv:2408.03615v1 [cs.AI] for this version)\nhttps://doi.org/10.48550/arXiv.2408.03615\nFocus to learn more\narXiv-issued DOI via DataCite (pending registration)\nSubmission history\nFrom: Zaijing Li [view email]\n[v1] Wed, 7 Aug 2024 08:16:32 UTC (48,016 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks, by Zaijing Li and 5 other authors\n* View PDF\n* HTML (experimental)\n* TeX Source\n* Other Formats\nview license\nCurrent browse context:\ncs.AI\n< prev | next >\nnew | recent | 2024-08\nChange to browse by:\ncs\ncs.CL\nReferences & Citations\n* NASA ADS\n* Google Scholar\n* Semantic Scholar\na export BibTeX citation Loading...\nBibTeX formatted citation\n×\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub Toggle\nDagsHub (What is DagsHub?)\nGotitPub Toggle\nGotit.pub (What is GotitPub?)\nLinks to Code Toggle\nPapers with Code (What is Papers with Code?)\nScienceCast Toggle\nScienceCast (What is ScienceCast?)\nDemos\nDemos\nReplicate Toggle\nReplicate (What is Replicate?)\nSpaces Toggle\nHugging Face Spaces (What is Spaces?)\nSpaces Toggle\nTXYZ.AI (What is TXYZ.AI?)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower (What are Influence Flowers?)\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nCore recommender toggle\nCORE Recommender (What is CORE?)\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n* About\n* Help\n* Click here to contact arXiv Contact\n* Click here to subscribe Subscribe\n* Copyright\n* Privacy Policy\n* Web Accessibility Assistance\n* arXiv Operational Status\nGet status notifications via email or slack",
      "summary": "Aiming to overcome limitations in artificial intelligence involving long-horizon tasks, the paper introduces a Hybrid Multimodal Memory module. This module transforms knowledge into a Hierarchical Directed Knowledge Graph for explicit learning and compiles historical information into an Abstracted Multimodal Experience Pool, enhancing agents' in-context learning capabilities. Built on this memory framework, the Optimus-1 agent employs a Knowledge-guided Planner and Experience-Driven Reflector, demonstrating superior performance on long-horizon tasks in Minecraft compared to existing agents and achieving near-human capabilities on several tasks. Additionally, Optimus-1, which incorporates Multimodal Large Language Models (MLLMs) as its backbone, shows improved generalization, outperforming the GPT-4V baseline in various tasks."
    }
  ]
  ```

  - Application Dimension
  ```json 

  ```

  - Scalability and Efficiency
  ```json 

  ```

  - Impact Dimension
  ```json 

  ```
